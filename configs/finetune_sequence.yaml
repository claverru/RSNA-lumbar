# lightning.pytorch==2.2.5
seed_everything: 821
trainer:
  max_epochs: 4
  callbacks:
  - class_path: lightning.pytorch.callbacks.ModelCheckpoint
    init_args:
      dirpath: null
      filename: '{epoch}_{val_loss:.4f}'
      monitor: val_loss
      verbose: false
      save_last: null
      save_top_k: 1
      save_weights_only: false
      mode: min
      auto_insert_metric_name: true
      every_n_train_steps: null
      train_time_interval: null
      every_n_epochs: null
      save_on_train_epoch_end: null
      enable_version_counter: true
  - class_path: lightning.pytorch.callbacks.LearningRateMonitor
    init_args:
      logging_interval: epoch
      log_momentum: false
      log_weight_decay: false
  - class_path: src.callbacks.Writer
    init_args:
      out_dir: null
      preds_name: preds.parquet
  - class_path: src.callbacks.CustomBackboneFinetuning
    init_args:
      unfreeze_at_epoch: 0
      unfreeze_bn: true
      backbone_names:
      - backbone
      - meta_proj
      - proj
      - transformer
      - heads
      - cls_emb
model:
  class_path: src.sequence.model.LightningModule
  init_args:
    train_any_severe_spinal: true
    optimizer:
      class_path: src.optimizers.Ranger
      init_args:
        lr: 1.0e-05
    lr_scheduler:
      class_path: src.model.WarmupLR
      init_args:
        warmup_steps: 98
        last_epoch: -1
    ckpt_path: lightning_logs/version_1/checkpoints/epoch=7_val_loss=0.3716.ckpt
data:
  class_path: src.sequence.data_loading.DataModule
  init_args:
    train_level_side: false
    batch_size: 4
